{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from objdet.YOLOV8.yolo import *\n",
    "\n",
    "model = yolo_v8_n()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shawn/anaconda3/envs/torch2/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x= torch.randn(1,3,640,640)\n",
    "\n",
    "before_eval = model(x)\n",
    "\n",
    "model.eval()\n",
    "after_eval = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 84, 8400])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after_eval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 84, 80, 80])\n",
      "torch.Size([1, 84, 40, 40])\n",
      "torch.Size([1, 84, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "for p in before_eval:\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9768, 1.0896, 1.2533, 1.1957, 1.3039, 1.2468, 1.2884, 1.2497, 1.2507,\n",
      "        1.4928, 1.0433, 1.3386, 1.0496, 1.2606, 0.9307, 0.7794, 0.5714, 0.9071,\n",
      "        1.6133, 1.4488, 1.5370, 1.7009, 1.0592, 1.5892, 1.2035, 1.1415, 1.5895,\n",
      "        1.3423, 1.5341, 1.2886, 1.3876, 1.1958, 1.1564, 1.1063, 1.3816, 1.2198,\n",
      "        1.1402, 1.0430, 0.9471, 1.2430, 1.2217, 1.3149, 1.4910, 1.4101, 0.8597,\n",
      "        0.8719, 0.9406, 1.1072, 1.1595, 1.1186, 0.9569, 0.9380, 0.8152, 1.1794,\n",
      "        1.3380, 1.4011, 1.1135, 1.0522, 1.0368, 1.0796, 0.9810, 1.0320, 0.8659,\n",
      "        1.0675, 1.0642, 1.0022, 1.1772, 1.0927, 1.3046, 1.3729, 1.4304, 1.2504,\n",
      "        1.0085, 1.1285, 1.0032, 1.2136, 1.2566, 1.1878, 1.0308, 0.9749],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([1.0762, 1.0786, 1.0857, 0.8224, 1.2243, 1.0502, 1.4101, 1.1686, 1.3602,\n",
      "        1.2024, 0.7550, 1.2048, 1.3575, 1.1047, 1.0687, 1.1626, 1.1720, 1.1908,\n",
      "        1.1288, 1.0056, 1.2489, 1.2540, 1.0954, 0.9124, 0.9609, 0.9880, 0.9556,\n",
      "        0.9237, 1.1818, 1.1349, 1.3661, 0.8549, 0.8645, 0.9490, 0.8551, 0.9526,\n",
      "        0.8860, 1.0480, 0.9214, 0.9606], grad_fn=<SelectBackward0>)\n",
      "tensor([1.0000, 1.1580, 0.8517, 0.8550, 0.8823, 1.0342, 1.0457, 1.0530, 1.0538,\n",
      "        0.7235, 1.2549, 0.7722, 0.9855, 1.1336, 0.9744, 1.1118, 1.3308, 0.9716,\n",
      "        1.0705, 0.7214], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(before_eval[0][0][0][0])\n",
    "print(before_eval[1][0][0][0])\n",
    "print(before_eval[2][0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  4.0065,  12.0005,  20.0014,  ..., 560.1067, 592.1022, 624.1115],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after_eval[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8400"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "80**2 + 40**2 + 20**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(\n",
    "    prediction,\n",
    "    conf_thres=0.25,\n",
    "    iou_thres=0.45,\n",
    "    classes=None,\n",
    "    agnostic=False,\n",
    "    multi_label=False,\n",
    "    labels=(),\n",
    "    max_det=300,\n",
    "    nc=0,  # number of classes (optional)\n",
    "    max_time_img=0.05,\n",
    "    max_nms=30000,\n",
    "    max_wh=7680,\n",
    "    in_place=True,\n",
    "    rotated=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform non-maximum suppression (NMS) on a set of boxes, with support for masks and multiple labels per box.\n",
    "\n",
    "    Args:\n",
    "        prediction (torch.Tensor): A tensor of shape (batch_size, num_classes + 4 + num_masks, num_boxes)\n",
    "            containing the predicted boxes, classes, and masks. The tensor should be in the format\n",
    "            output by a model, such as YOLO.\n",
    "        conf_thres (float): The confidence threshold below which boxes will be filtered out.\n",
    "            Valid values are between 0.0 and 1.0.\n",
    "        iou_thres (float): The IoU threshold below which boxes will be filtered out during NMS.\n",
    "            Valid values are between 0.0 and 1.0.\n",
    "        classes (List[int]): A list of class indices to consider. If None, all classes will be considered.\n",
    "        agnostic (bool): If True, the model is agnostic to the number of classes, and all\n",
    "            classes will be considered as one.\n",
    "        multi_label (bool): If True, each box may have multiple labels.\n",
    "        labels (List[List[Union[int, float, torch.Tensor]]]): A list of lists, where each inner\n",
    "            list contains the apriori labels for a given image. The list should be in the format\n",
    "            output by a dataloader, with each label being a tuple of (class_index, x1, y1, x2, y2).\n",
    "        max_det (int): The maximum number of boxes to keep after NMS.\n",
    "        nc (int, optional): The number of classes output by the model. Any indices after this will be considered masks.\n",
    "        max_time_img (float): The maximum time (seconds) for processing one image.\n",
    "        max_nms (int): The maximum number of boxes into torchvision.ops.nms().\n",
    "        max_wh (int): The maximum box width and height in pixels.\n",
    "        in_place (bool): If True, the input prediction tensor will be modified in place.\n",
    "\n",
    "    Returns:\n",
    "        (List[torch.Tensor]): A list of length batch_size, where each element is a tensor of\n",
    "            shape (num_boxes, 6 + num_masks) containing the kept boxes, with columns\n",
    "            (x1, y1, x2, y2, confidence, class, mask1, mask2, ...).\n",
    "    \"\"\"\n",
    "    import torchvision  # scope for faster 'import ultralytics'\n",
    "\n",
    "    # Checks\n",
    "    assert 0 <= conf_thres <= 1, f\"Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0\"\n",
    "    assert 0 <= iou_thres <= 1, f\"Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0\"\n",
    "    if isinstance(prediction, (list, tuple)):  # YOLOv8 model in validation model, output = (inference_out, loss_out)\n",
    "        prediction = prediction[0]  # select only inference output\n",
    "\n",
    "    bs = prediction.shape[0]  # batch size\n",
    "    nc = nc or (prediction.shape[1] - 4)  # number of classes\n",
    "    nm = prediction.shape[1] - nc - 4\n",
    "    mi = 4 + nc  # mask start index\n",
    "    xc = prediction[:, 4:mi].amax(1) > conf_thres  # candidates\n",
    "\n",
    "    # Settings\n",
    "    # min_wh = 2  # (pixels) minimum box width and height\n",
    "    time_limit = 2.0 + max_time_img * bs  # seconds to quit after\n",
    "    multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
    "\n",
    "    prediction = prediction.transpose(-1, -2)  # shape(1,84,6300) to shape(1,6300,84)\n",
    "    if not rotated:\n",
    "        if in_place:\n",
    "            prediction[..., :4] = xywh2xyxy(prediction[..., :4])  # xywh to xyxy\n",
    "        else:\n",
    "            prediction = torch.cat((xywh2xyxy(prediction[..., :4]), prediction[..., 4:]), dim=-1)  # xywh to xyxy\n",
    "\n",
    "    t = time.time()\n",
    "    output = [torch.zeros((0, 6 + nm), device=prediction.device)] * bs\n",
    "    for xi, x in enumerate(prediction):  # image index, image inference\n",
    "        # Apply constraints\n",
    "        # x[((x[:, 2:4] < min_wh) | (x[:, 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
    "        x = x[xc[xi]]  # confidence\n",
    "\n",
    "        # Cat apriori labels if autolabelling\n",
    "        if labels and len(labels[xi]) and not rotated:\n",
    "            lb = labels[xi]\n",
    "            v = torch.zeros((len(lb), nc + nm + 4), device=x.device)\n",
    "            v[:, :4] = xywh2xyxy(lb[:, 1:5])  # box\n",
    "            v[range(len(lb)), lb[:, 0].long() + 4] = 1.0  # cls\n",
    "            x = torch.cat((x, v), 0)\n",
    "\n",
    "        # If none remain process next image\n",
    "        if not x.shape[0]:\n",
    "            continue\n",
    "\n",
    "        # Detections matrix nx6 (xyxy, conf, cls)\n",
    "        box, cls, mask = x.split((4, nc, nm), 1)\n",
    "\n",
    "        if multi_label:\n",
    "            i, j = torch.where(cls > conf_thres)\n",
    "            x = torch.cat((box[i], x[i, 4 + j, None], j[:, None].float(), mask[i]), 1)\n",
    "        else:  # best class only\n",
    "            conf, j = cls.max(1, keepdim=True)\n",
    "            x = torch.cat((box, conf, j.float(), mask), 1)[conf.view(-1) > conf_thres]\n",
    "\n",
    "        # Filter by class\n",
    "        if classes is not None:\n",
    "            x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
    "\n",
    "        # Check shape\n",
    "        n = x.shape[0]  # number of boxes\n",
    "        if not n:  # no boxes\n",
    "            continue\n",
    "        if n > max_nms:  # excess boxes\n",
    "            x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence and remove excess boxes\n",
    "\n",
    "        # Batched NMS\n",
    "        c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
    "        scores = x[:, 4]  # scores\n",
    "        if rotated:\n",
    "            boxes = torch.cat((x[:, :2] + c, x[:, 2:4], x[:, -1:]), dim=-1)  # xywhr\n",
    "            i = nms_rotated(boxes, scores, iou_thres)\n",
    "        else:\n",
    "            boxes = x[:, :4] + c  # boxes (offset by class)\n",
    "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
    "        i = i[:max_det]  # limit detections\n",
    "\n",
    "        # # Experimental\n",
    "        # merge = False  # use merge-NMS\n",
    "        # if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
    "        #     # Update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
    "        #     from .metrics import box_iou\n",
    "        #     iou = box_iou(boxes[i], boxes) > iou_thres  # IoU matrix\n",
    "        #     weights = iou * scores[None]  # box weights\n",
    "        #     x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
    "        #     redundant = True  # require redundant detections\n",
    "        #     if redundant:\n",
    "        #         i = i[iou.sum(1) > 1]  # require redundancy\n",
    "\n",
    "        output[xi] = x[i]\n",
    "        if (time.time() - t) > time_limit:\n",
    "            LOGGER.warning(f\"WARNING ⚠️ NMS time limit {time_limit:.3f}s exceeded\")\n",
    "            break  # time limit exceeded\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xywh2xyxy(x):\n",
    "    \"\"\"\n",
    "    Convert bounding box coordinates from (x, y, width, height) format to (x1, y1, x2, y2) format where (x1, y1) is the\n",
    "    top-left corner and (x2, y2) is the bottom-right corner.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray | torch.Tensor): The input bounding box coordinates in (x, y, width, height) format.\n",
    "\n",
    "    Returns:\n",
    "        y (np.ndarray | torch.Tensor): The bounding box coordinates in (x1, y1, x2, y2) format.\n",
    "    \"\"\"\n",
    "    assert x.shape[-1] == 4, f\"input shape last dimension expected 4 but input shape is {x.shape}\"\n",
    "    y = torch.empty_like(x) if isinstance(x, torch.Tensor) else np.empty_like(x)  # faster than clone/copy\n",
    "    dw = x[..., 2] / 2  # half-width\n",
    "    dh = x[..., 3] / 2  # half-height\n",
    "    y[..., 0] = x[..., 0] - dw  # top left x\n",
    "    y[..., 1] = x[..., 1] - dh  # top left y\n",
    "    y[..., 2] = x[..., 0] + dw  # bottom right x\n",
    "    y[..., 3] = x[..., 1] + dh  # bottom right y\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50, 50, 20, 20],\n",
       "        [30, 30, 10, 10]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([\n",
    "    [50, 50, 20, 20],  # 바운딩 박스 1\n",
    "    [30, 30, 10, 10]   # 바운딩 박스 2\n",
    "])\n",
    "x[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([], size=(0, 6))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "non_max_suppression(after_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-9.9736e+00, -9.9799e+00,  2.0175e+00,  ...,  9.8039e-06,  9.7436e-06,  9.7837e-06],\n",
       "         [-5.9689e+00, -9.9772e+00,  1.4011e+01,  ...,  9.8075e-06,  9.7336e-06,  9.7902e-06],\n",
       "         [-1.9691e+00, -9.9771e+00,  2.6012e+01,  ...,  9.8068e-06,  9.7336e-06,  9.7897e-06],\n",
       "         ...,\n",
       "         [ 2.3232e+02,  2.6400e+02,  8.2425e+02,  ...,  1.5653e-04,  1.5539e-04,  1.5598e-04],\n",
       "         [ 2.4831e+02,  2.6400e+02,  8.7224e+02,  ...,  1.5652e-04,  1.5541e-04,  1.5601e-04],\n",
       "         [ 2.6433e+02,  2.6400e+02,  9.2026e+02,  ...,  1.5643e-04,  1.5532e-04,  1.5603e-04]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after_eval.shape\n",
    "a= after_eval.transpose(-1,-2)\n",
    "a.shape\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8400, 4])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[..., :4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('/mnt/d/projects/objdet/src/objdet/YOLOV8/yolov8n.pt')\n",
    "\n",
    "state_dict['model'].state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# YOLOv8 모델 초기화\n",
    "model = yolo_v8_n()\n",
    "\n",
    "# 가중치 파일 경로\n",
    "weight_path = 'yolov8n.pt'\n",
    "\n",
    "# 가중치 로드\n",
    "checkpoint = torch.load(weight_path)\n",
    "\n",
    "# 불필요한 키 제거\n",
    "if 'model' in checkpoint:\n",
    "    state_dict = checkpoint['model'].state_dict()\n",
    "else:\n",
    "    state_dict = checkpoint\n",
    "\n",
    "# 필요한 키만 남기기\n",
    "filtered_state_dict = {}\n",
    "model_keys = model.state_dict().keys()\n",
    "for k, v in state_dict.items():\n",
    "    if k in model_keys:\n",
    "        filtered_state_dict[k] = v\n",
    "\n",
    "# 가중치를 모델에 로드\n",
    "model.load_state_dict(filtered_state_dict, strict=False)\n",
    "\n",
    "# 모델을 평가 모드로 전환\n",
    "model.eval()\n",
    "\n",
    "# # 모델 테스트 (예시 입력)\n",
    "# x = torch.randn(1, 3, 640, 640)\n",
    "# output = model(x)\n",
    "# print(output.shape)\n",
    "\n",
    "# # 가중치 숫자 값 확인\n",
    "# for name, param in filtered_state_dict.items():\n",
    "#     print(f\"{name}: {param.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "image_path = '/mnt/d/projects/objdet/src/objdet/samples/sample.png'\n",
    "import numpy as np \n",
    "\n",
    "img = cv2.imread(image_path)\n",
    "H,W,C = img.shape\n",
    "img = img.astype(np.float32)[:,:,::-1]\n",
    "img = img/255\n",
    "img = img.transpose((2,0,1))\n",
    "img = torch.from_numpy(img).half().unsqueeze(0).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = checkpoint['model']\n",
    "m.cuda()\n",
    "m.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = m(img)\n",
    "output[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 144, 72, 96])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = output[1][0].reshape(1,144,72*96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 144, 6912])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([], device='cuda:0', size=(0, 6))]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_max_suppression(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = non_max_suppression(output, conf_thres = 0.1, iou_thres=0.5)[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    -227.25,      -393.5,     -107.75,      -197.5,     0.92627,          16],\n",
       "       [     -361.5,     -288.25,      -453.5,     -235.75,      0.8623,           1],\n",
       "       [       -384,      -288.5,        -343,      -234.5,     0.81982,           1],\n",
       "       [    -380.75,     -289.75,     -362.25,     -231.25,     0.77734,           1],\n",
       "       [     -371.5,     -289.25,      -406.5,     -234.75,     0.77588,           1],\n",
       "       [       -548,        -126,      -17.75,     -56.156,     0.76758,           2],\n",
       "       [    -362.25,        -290,     -452.75,        -239,     0.76367,           1],\n",
       "       [    -372.25,     -288.25,     -397.75,     -234.75,     0.50586,           1],\n",
       "       [       -375,      -290.5,        -389,      -236.5,     0.34692,           1],\n",
       "       [       -376,        -289,        -379,        -237,     0.33105,           1],\n",
       "       [    -194.75,        -270,      -68.25,        -150,     0.17554,           1],\n",
       "       [    -51.594,     -155.62,     -72.125,     -216.62,     0.11359,          62]], dtype=float32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[1.3150e+02, 2.2100e+02, 3.1100e+02, 5.4150e+02, 9.2627e-01, 1.6000e+01],\n",
      "        [3.3000e+01, 1.0475e+02, 5.6550e+02, 4.1925e+02, 8.6230e-01, 1.0000e+00],\n",
      "        [4.6850e+02, 7.5125e+01, 6.8950e+02, 1.7150e+02, 7.6758e-01, 2.0000e+00],\n",
      "        [1.2775e+02, 1.4250e+02, 2.6050e+02, 3.7500e+02, 1.7554e-01, 1.0000e+00],\n",
      "        [6.2500e-02, 7.5000e-01, 8.2562e+01, 2.4862e+02, 1.1359e-01, 6.2000e+01]], device='cuda:0')]\n",
      "[[      131.5         221         311       541.5     0.92627          16]\n",
      " [         33      104.75       565.5      419.25      0.8623           1]\n",
      " [      468.5      75.125       689.5       171.5     0.76758           2]\n",
      " [     127.75       142.5       260.5         375     0.17554           1]\n",
      " [     0.0625        0.75      82.562      248.62     0.11359          62]]\n"
     ]
    }
   ],
   "source": [
    "print(output)\n",
    "print(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 이미지 로드\n",
    "image_path = '/mnt/d/projects/objdet/src/objdet/samples/sample.png'\n",
    "img = cv2.imread(image_path)\n",
    "H, W, C = img.shape\n",
    "\n",
    "# 모델의 출력 예시 (이미지가 640x640으로 스케일링된 경우를 가정)\n",
    "output = [\n",
    "    torch.tensor(\n",
    "        [\n",
    "            [1.3150e+02, 2.2100e+02, 3.1100e+02, 5.4150e+02, 9.2627e-01, 1.6000e+01],\n",
    "            [3.3000e+01, 1.0475e+02, 5.6550e+02, 4.1925e+02, 8.6230e-01, 1.0000e+00],\n",
    "            [4.6850e+02, 7.5125e+01, 6.8950e+02, 1.7150e+02, 7.6758e-01, 2.0000e+00],\n",
    "            [1.2775e+02, 1.4250e+02, 2.6050e+02, 3.7500e+02, 1.7554e-01, 1.0000e+00],\n",
    "            [6.2500e-02, 7.5000e-01, 8.2562e+01, 2.4862e+02, 1.1359e-01, 6.2000e+01],\n",
    "        ],\n",
    "        device='cuda:0'\n",
    "    )\n",
    "]\n",
    "\n",
    "# Convert the output to CPU and numpy\n",
    "boxes = output[0].cpu().numpy()\n",
    "\n",
    "# 이미지 스케일을 고려하여 박스 좌표를 변환\n",
    "img_scale = 640  # 모델 입력 이미지 크기 (예시로 640을 사용)\n",
    "scale_w = W / img_scale\n",
    "scale_h = H / img_scale\n",
    "\n",
    "# Draw boxes on the original image\n",
    "for box in boxes:\n",
    "    x1, y1, x2, y2, conf, cls = box\n",
    "    x1, y1, x2 ,y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "    # x1, y1, x2, y2 = int(x1 * W), int(y1 * H), int(x2 * W), int(y2 * H)\n",
    "\n",
    "    # Check if the box coordinates are valid\n",
    "    print(f'Drawing box: ({x1}, {y1}, {x2}, {y2}) with confidence: {conf}, class: {cls}')\n",
    "    \n",
    "    # Draw rectangle\n",
    "    cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    \n",
    "    # Put label\n",
    "    label = f'{int(cls)}: {conf:.2f}'\n",
    "    cv2.putText(img, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "# Display the image\n",
    "cv2.imshow('Image with Boxes', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "m = YOLO('yolov8n.pt')\n",
    "image_path = '/mnt/d/projects/objdet/src/objdet/samples/sample.png'\n",
    "\n",
    "m.predict(image_path,save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(image_path)\n",
    "\n",
    "# Draw boxes on the original image\n",
    "for box in boxes:\n",
    "    x1, y1, x2, y2, conf, cls = box\n",
    "    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "    \n",
    "    # Draw rectangle\n",
    "    cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    \n",
    "    # Put label\n",
    "    label = f'{int(cls)}: {conf:.2f}'\n",
    "    cv2.putText(img, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "# Display the image\n",
    "cv2.imshow('Image with Boxes', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
