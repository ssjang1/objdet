{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "from objdet.YOLOV8.utils.tal import make_anchors\n",
    "from objdet.YOLOV8.nn.modules.conv import Conv, Concat\n",
    "from objdet.YOLOV8.nn.modules.block import DFL, C2f, SPPF\n",
    "from objdet.YOLOV8.nn.modules.head import Detect\n",
    "\n",
    "\n",
    "def fuse_conv(conv, norm):\n",
    "    fused_conv = torch.nn.Conv2d(conv.in_channels,\n",
    "                                 conv.out_channels,\n",
    "                                 kernel_size=conv.kernel_size,\n",
    "                                 stride=conv.stride,\n",
    "                                 padding=conv.padding,\n",
    "                                 groups=conv.groups,\n",
    "                                 bias=True).requires_grad_(False).to(conv.weight.device)\n",
    "\n",
    "    w_conv = conv.weight.clone().view(conv.out_channels, -1)\n",
    "    w_norm = torch.diag(norm.weight.div(torch.sqrt(norm.eps + norm.running_var)))\n",
    "    fused_conv.weight.copy_(torch.mm(w_norm, w_conv).view(fused_conv.weight.size()))\n",
    "\n",
    "    b_conv = torch.zeros(conv.weight.size(0), device=conv.weight.device) if conv.bias is None else conv.bias\n",
    "    b_norm = norm.bias - norm.weight.mul(norm.running_mean).div(torch.sqrt(norm.running_var + norm.eps))\n",
    "    fused_conv.bias.copy_(torch.mm(w_norm, b_conv.reshape(-1, 1)).reshape(-1) + b_norm)\n",
    "\n",
    "    return fused_conv\n",
    "\n",
    "\n",
    "\n",
    "class DarkNet(torch.nn.Module):\n",
    "    def __init__(self, width, depth):\n",
    "        super().__init__()\n",
    "        self.p1 = []\n",
    "        self.p2 = []\n",
    "        self.p3 = []\n",
    "        self.p4 = []\n",
    "        self.p5 = []\n",
    "\n",
    "        # p1/2\n",
    "        self.p1.append(Conv(width[0], width[1], k=3, s=2, p=1))\n",
    "        # p2/4\n",
    "        self.p2.append(Conv(width[1], width[2], k=3, s=2, p=1))\n",
    "        self.p2.append(C2f(width[2], width[2], depth[0]))\n",
    "        # p3/8\n",
    "        self.p3.append(Conv(width[2], width[3], k=3, s=2, p=1))\n",
    "        self.p3.append(C2f(width[3], width[3], depth[1]))\n",
    "        # p4/16\n",
    "        self.p4.append(Conv(width[3], width[4], k=3, s=2, p=1))\n",
    "        self.p4.append(C2f(width[4], width[4], depth[2]))\n",
    "        # p5/32\n",
    "        self.p5.append(Conv(width[4], width[5], k=3, s=2, p=1))\n",
    "        self.p5.append(C2f(width[5], width[5], depth[0]))\n",
    "        self.p5.append(SPPF(width[5], width[5]))\n",
    "\n",
    "        self.p1 = torch.nn.Sequential(*self.p1)\n",
    "        self.p2 = torch.nn.Sequential(*self.p2)\n",
    "        self.p3 = torch.nn.Sequential(*self.p3)\n",
    "        self.p4 = torch.nn.Sequential(*self.p4)\n",
    "        self.p5 = torch.nn.Sequential(*self.p5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p1 = self.p1(x)\n",
    "        p2 = self.p2(p1)\n",
    "        p3 = self.p3(p2)\n",
    "        p4 = self.p4(p3)\n",
    "        p5 = self.p5(p4)\n",
    "        return p3, p4, p5\n",
    "\n",
    "\n",
    "class DarkFPN(torch.nn.Module):\n",
    "    def __init__(self, width, depth):\n",
    "        super().__init__()\n",
    "        self.up = torch.nn.Upsample(scale_factor=2)\n",
    "        self.h1 = C2f(width[4] + width[5], width[4], depth[0])\n",
    "        self.h2 = C2f(width[3] + width[4], width[3], depth[0])\n",
    "        self.h3 = Conv(width[3], width[3], k=3, s=2, p=1)\n",
    "        self.h4 = C2f(width[3] + width[4], width[4], depth[0])\n",
    "        self.h5 = Conv(width[4], width[4], k=3, s=2, p=1)\n",
    "        self.h6 = C2f(width[4] + width[5], width[5], depth[0])\n",
    "\n",
    "    def forward(self, x):\n",
    "        p3, p4, p5 = x\n",
    "        p4 = self.h1(torch.cat(tensors=[self.up(p5), p4], dim=1))\n",
    "        p3 = self.h2(torch.cat(tensors=[self.up(p4), p3], dim=1))\n",
    "        p4 = self.h4(torch.cat(tensors=[self.h3(p3), p4], dim=1))\n",
    "        p5 = self.h6(torch.cat(tensors=[self.h5(p4), p5], dim=1))\n",
    "        return p3, p4, p5\n",
    "\n",
    "\n",
    "class Head(torch.nn.Module):\n",
    "    shape = None\n",
    "    anchors = torch.empty(0)\n",
    "    strides = torch.empty(0)\n",
    "\n",
    "    def __init__(self, nc=80, filters=()):\n",
    "        super().__init__()\n",
    "        self.nc = nc  # number of classes\n",
    "        self.no = nc + 4  # number of outputs per anchor\n",
    "        self.stride = torch.zeros(len(filters))  # strides computed during build\n",
    "\n",
    "        box = max(64, filters[0] // 4)\n",
    "        cls = max(80, filters[0], self.nc)\n",
    "\n",
    "        self.box = torch.nn.ModuleList(torch.nn.Sequential(Conv(x, box, k=3, p=1),\n",
    "                                                           Conv(box, box, k=3, p=1),\n",
    "                                                           torch.nn.Conv2d(box, out_channels=4,\n",
    "                                                                           kernel_size=1)) for x in filters)\n",
    "        self.cls = torch.nn.ModuleList(torch.nn.Sequential(Conv(x, cls, k=3, p=1),\n",
    "                                                           Conv(cls, cls, k=3, p=1),\n",
    "                                                           torch.nn.Conv2d(cls, out_channels=self.nc,\n",
    "                                                                           kernel_size=1)) for x in filters)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shape = x[0].shape\n",
    "        for i, (box, cls) in enumerate(zip(self.box, self.cls)):\n",
    "            x[i] = torch.cat(tensors=(box(x[i]), cls(x[i])), dim=1)\n",
    "        if self.training:\n",
    "            return x\n",
    "        if self.shape != shape:\n",
    "            self.shape = shape\n",
    "            self.anchors, self.strides = (i.transpose(0, 1) for i in make_anchors(x, self.stride))\n",
    "\n",
    "        x = torch.cat([i.view(x[0].shape[0], self.no, -1) for i in x], dim=2)\n",
    "        box, cls = x.split(split_size=(4, self.nc), dim=1)\n",
    "\n",
    "        a, b = box.chunk(2, 1)\n",
    "        a = self.anchors.unsqueeze(0) - a\n",
    "        b = self.anchors.unsqueeze(0) + b\n",
    "        box = torch.cat(tensors=((a + b) / 2, b - a), dim=1)\n",
    "\n",
    "        return torch.cat(tensors=(box * self.strides, cls.sigmoid()), dim=1)\n",
    "\n",
    "    def initialize_biases(self):\n",
    "        # Initialize biases\n",
    "        # WARNING: requires stride availability\n",
    "        for box, cls, s in zip(self.box, self.cls, self.stride):\n",
    "            # box\n",
    "            box[-1].bias.data[:] = 1.0\n",
    "            # cls (.01 objects, 80 classes, 640 img)\n",
    "            cls[-1].bias.data[:self.nc] = math.log(5 / self.nc / (640 / s) ** 2)\n",
    "\n",
    "\n",
    "class YOLO(torch.nn.Module):\n",
    "    def __init__(self, width, depth, num_classes):\n",
    "        super().__init__()\n",
    "        self.net = DarkNet(width, depth)\n",
    "        self.fpn = DarkFPN(width, depth)\n",
    "\n",
    "        img_dummy = torch.zeros(1, width[0], 256, 256)\n",
    "        self.head = Head(num_classes, (width[3], width[4], width[5]))\n",
    "        self.head.stride = torch.tensor([256 / x.shape[-2] for x in self.forward(img_dummy)])\n",
    "        self.stride = self.head.stride\n",
    "        self.head.initialize_biases()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        x = self.fpn(x)\n",
    "        return self.head(list(x))\n",
    "\n",
    "    def fuse(self):\n",
    "        for m in self.modules():\n",
    "            if type(m) is Conv and hasattr(m, 'norm'):\n",
    "                m.conv = fuse_conv(m.conv, m.norm)\n",
    "                m.forward = m.fuse_forward\n",
    "                delattr(m, 'norm')\n",
    "        return self\n",
    "\n",
    "\n",
    "def yolo_v8_n(num_classes: int = 80):\n",
    "    depth = [1, 2, 2]\n",
    "    width = [3, 16, 32, 64, 128, 256]\n",
    "    return YOLO(width, depth, num_classes)\n",
    "\n",
    "\n",
    "def yolo_v8_t(num_classes: int = 80):\n",
    "    depth = [1, 2, 2]\n",
    "    width = [3, 24, 48, 96, 192, 384]\n",
    "    return YOLO(width, depth, num_classes)\n",
    "\n",
    "\n",
    "def yolo_v8_s(num_classes: int = 80):\n",
    "    depth = [1, 2, 2]\n",
    "    width = [3, 32, 64, 128, 256, 512]\n",
    "    return YOLO(width, depth, num_classes)\n",
    "\n",
    "\n",
    "def yolo_v8_m(num_classes: int = 80):\n",
    "    depth = [2, 4, 4]\n",
    "    width = [3, 48, 96, 192, 384, 576]\n",
    "    return YOLO(width, depth, num_classes)\n",
    "\n",
    "\n",
    "def yolo_v8_l(num_classes: int = 80):\n",
    "    depth = [3, 6, 6]\n",
    "    width = [3, 64, 128, 256, 512, 512]\n",
    "    return YOLO(width, depth, num_classes)\n",
    "\n",
    "\n",
    "def yolo_v8_x(num_classes: int = 80):\n",
    "    depth = [3, 6, 6]\n",
    "    width = [3, 80, 160, 320, 640, 640]\n",
    "    return YOLO(width, depth, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/mnt/d/projects/objdet/src/objdet/samples/sample.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(576, 768, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "img = cv2.imread(image_path)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(576, 768, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def input_transform(image):\n",
    "    image = image.astype(np.float32)[:, :, ::-1]\n",
    "    image = image / 255.0\n",
    "    return image\n",
    "\n",
    "img2= input_transform(img)\n",
    "img2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = cv2.imread(file_path, cv2.IMREAD_COLOR)\n",
    "# height, width, _ = img.shape\n",
    "# padded_img = pad_image(img)\n",
    "# padded_height, padded_width, _ = padded_img.shape\n",
    "\n",
    "# sv_img = np.zeros_like(padded_img).astype(np.uint8)\n",
    "# img_transformed = input_transform(padded_img)\n",
    "# img_transformed = img_transformed.transpose((2, 0, 1)).copy()\n",
    "# img_tensor = torch.from_numpy(img_transformed).unsqueeze(0).cuda()\n",
    "\n",
    "# pred = model(img_tensor)\n",
    "# pred = F.interpolate(pred, size=(padded_height, padded_width), mode='bilinear', align_corners=True)\n",
    "# pred = torch.argmax(pred, dim=1).squeeze(0).cpu().numpy()\n",
    "# pred = pred[:height, :width]  # 패딩된 부분을 제거하여 원본 크기로 되돌림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 576, 768)\n",
      "torch.Size([1, 3, 576, 768])\n"
     ]
    }
   ],
   "source": [
    "img3 = img2.transpose((2,0,1))\n",
    "print(img3.shape)\n",
    "img4 = torch.from_numpy(img3).unsqueeze(0).cuda()\n",
    "print(img4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = yolo_v8_n().cuda()\n",
    "prediction = model(img4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 84, 72, 96])\n",
      "torch.Size([1, 84, 36, 48])\n",
      "torch.Size([1, 84, 18, 24])\n"
     ]
    }
   ],
   "source": [
    "for p in prediction:\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(576, 768, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 576, 768])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([84, 18, 24])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[2][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 84, 72, 96])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 576, 768])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(im)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(im\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28mprint\u001b[39m(pred\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "image_path = '/mnt/d/projects/objdet/src/objdet/samples/sample.png'\n",
    "im = cv2.imread(image_path)\n",
    "im0 = im.copy()\n",
    "im = im.astype(np.float32)[:,:,::-1]\n",
    "im = im/255\n",
    "im = im.transpose((2,0,1))\n",
    "im = torch.from_numpy(im).unsqueeze(0)\n",
    "\n",
    "model = yolo_v8_n()\n",
    "\n",
    "pred = model(im)\n",
    "\n",
    "print(im.shape)\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 84, 72, 96])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([96768, 6])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(pred[0].shape)\n",
    "pred[0].reshape(-1,6).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 84, 72, 96])\n",
      "torch.Size([1, 84, 36, 48])\n",
      "torch.Size([1, 84, 18, 24])\n"
     ]
    }
   ],
   "source": [
    "for p in pred:\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = yolo_v8_x()\n",
    "\n",
    "pred2= m2(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 84, 72, 96])\n",
      "torch.Size([1, 84, 36, 48])\n",
      "torch.Size([1, 84, 18, 24])\n"
     ]
    }
   ],
   "source": [
    "for p in pred2:\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shawn/anaconda3/envs/torch2/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "result1= model(im)\n",
    "\n",
    "model.eval()\n",
    "result2 = model(im)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 84, 72, 96])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([127008, 6])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2.reshape(-1,6).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = result2.reshape(-1,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = array[:, :4]  # Bounding box coordinates\n",
    "confidences = array[:, 4]  # Confidence scores\n",
    "class_scores = array[:, 5]  # Class scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 84, 9072])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "image_path = '/mnt/d/projects/objdet/src/objdet/samples/sample.png'\n",
    "im = cv2.imread(image_path)\n",
    "im0 = im.copy()\n",
    "im = im.astype(np.float32)[:,:,::-1]\n",
    "im = im/255\n",
    "im = im.transpose((2,0,1))\n",
    "im = torch.from_numpy(im).unsqueeze(0)\n",
    "\n",
    "model = yolo_v8_n()\n",
    "\n",
    "pred = model(im)\n",
    "\n",
    "print(im.shape)\n",
    "print(pred.shape)\n",
    "\n",
    "model.eval()\n",
    "result2 = model(im)\n",
    "array = result2.reshape(-1,6)\n",
    "boxes = array[:, :4]  # Bounding box coordinates\n",
    "confidences = array[:, 4]  # Confidence scores\n",
    "class_scores = array[:, 5]  # Class scores\n",
    "# Assume 'boxes', 'confidences', and 'class_scores' are already defined\n",
    "indices = cv2.dnn.NMSBoxes(boxes.tolist(), confidences.tolist(), score_threshold=0.5, nms_threshold=0.4)\n",
    "\n",
    "final_boxes = boxes[indices].reshape(-1, 4)\n",
    "final_confidences = confidences[indices]\n",
    "final_class_scores = class_scores[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1151, 1166, 1165, 1164, 1163, 1441, 1140, 1139, 1161, 1440, 1719, 1687, 1137, 1160, 2956, 2687, 1136, 5975, 2672, 1559, 2952, 1543, 2671, 1527], dtype=int32)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(576, 768, 3)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load('yolov8n.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for YOLO:\n\tMissing key(s) in state_dict: \"net.p1.0.conv.weight\", \"net.p1.0.bn.weight\", \"net.p1.0.bn.bias\", \"net.p1.0.bn.running_mean\", \"net.p1.0.bn.running_var\", \"net.p2.0.conv.weight\", \"net.p2.0.bn.weight\", \"net.p2.0.bn.bias\", \"net.p2.0.bn.running_mean\", \"net.p2.0.bn.running_var\", \"net.p2.1.cv1.conv.weight\", \"net.p2.1.cv1.bn.weight\", \"net.p2.1.cv1.bn.bias\", \"net.p2.1.cv1.bn.running_mean\", \"net.p2.1.cv1.bn.running_var\", \"net.p2.1.cv2.conv.weight\", \"net.p2.1.cv2.bn.weight\", \"net.p2.1.cv2.bn.bias\", \"net.p2.1.cv2.bn.running_mean\", \"net.p2.1.cv2.bn.running_var\", \"net.p2.1.m.0.cv1.conv.weight\", \"net.p2.1.m.0.cv1.bn.weight\", \"net.p2.1.m.0.cv1.bn.bias\", \"net.p2.1.m.0.cv1.bn.running_mean\", \"net.p2.1.m.0.cv1.bn.running_var\", \"net.p2.1.m.0.cv2.conv.weight\", \"net.p2.1.m.0.cv2.bn.weight\", \"net.p2.1.m.0.cv2.bn.bias\", \"net.p2.1.m.0.cv2.bn.running_mean\", \"net.p2.1.m.0.cv2.bn.running_var\", \"net.p3.0.conv.weight\", \"net.p3.0.bn.weight\", \"net.p3.0.bn.bias\", \"net.p3.0.bn.running_mean\", \"net.p3.0.bn.running_var\", \"net.p3.1.cv1.conv.weight\", \"net.p3.1.cv1.bn.weight\", \"net.p3.1.cv1.bn.bias\", \"net.p3.1.cv1.bn.running_mean\", \"net.p3.1.cv1.bn.running_var\", \"net.p3.1.cv2.conv.weight\", \"net.p3.1.cv2.bn.weight\", \"net.p3.1.cv2.bn.bias\", \"net.p3.1.cv2.bn.running_mean\", \"net.p3.1.cv2.bn.running_var\", \"net.p3.1.m.0.cv1.conv.weight\", \"net.p3.1.m.0.cv1.bn.weight\", \"net.p3.1.m.0.cv1.bn.bias\", \"net.p3.1.m.0.cv1.bn.running_mean\", \"net.p3.1.m.0.cv1.bn.running_var\", \"net.p3.1.m.0.cv2.conv.weight\", \"net.p3.1.m.0.cv2.bn.weight\", \"net.p3.1.m.0.cv2.bn.bias\", \"net.p3.1.m.0.cv2.bn.running_mean\", \"net.p3.1.m.0.cv2.bn.running_var\", \"net.p3.1.m.1.cv1.conv.weight\", \"net.p3.1.m.1.cv1.bn.weight\", \"net.p3.1.m.1.cv1.bn.bias\", \"net.p3.1.m.1.cv1.bn.running_mean\", \"net.p3.1.m.1.cv1.bn.running_var\", \"net.p3.1.m.1.cv2.conv.weight\", \"net.p3.1.m.1.cv2.bn.weight\", \"net.p3.1.m.1.cv2.bn.bias\", \"net.p3.1.m.1.cv2.bn.running_mean\", \"net.p3.1.m.1.cv2.bn.running_var\", \"net.p4.0.conv.weight\", \"net.p4.0.bn.weight\", \"net.p4.0.bn.bias\", \"net.p4.0.bn.running_mean\", \"net.p4.0.bn.running_var\", \"net.p4.1.cv1.conv.weight\", \"net.p4.1.cv1.bn.weight\", \"net.p4.1.cv1.bn.bias\", \"net.p4.1.cv1.bn.running_mean\", \"net.p4.1.cv1.bn.running_var\", \"net.p4.1.cv2.conv.weight\", \"net.p4.1.cv2.bn.weight\", \"net.p4.1.cv2.bn.bias\", \"net.p4.1.cv2.bn.running_mean\", \"net.p4.1.cv2.bn.running_var\", \"net.p4.1.m.0.cv1.conv.weight\", \"net.p4.1.m.0.cv1.bn.weight\", \"net.p4.1.m.0.cv1.bn.bias\", \"net.p4.1.m.0.cv1.bn.running_mean\", \"net.p4.1.m.0.cv1.bn.running_var\", \"net.p4.1.m.0.cv2.conv.weight\", \"net.p4.1.m.0.cv2.bn.weight\", \"net.p4.1.m.0.cv2.bn.bias\", \"net.p4.1.m.0.cv2.bn.running_mean\", \"net.p4.1.m.0.cv2.bn.running_var\", \"net.p4.1.m.1.cv1.conv.weight\", \"net.p4.1.m.1.cv1.bn.weight\", \"net.p4.1.m.1.cv1.bn.bias\", \"net.p4.1.m.1.cv1.bn.running_mean\", \"net.p4.1.m.1.cv1.bn.running_var\", \"net.p4.1.m.1.cv2.conv.weight\", \"net.p4.1.m.1.cv2.bn.weight\", \"net.p4.1.m.1.cv2.bn.bias\", \"net.p4.1.m.1.cv2.bn.running_mean\", \"net.p4.1.m.1.cv2.bn.running_var\", \"net.p5.0.conv.weight\", \"net.p5.0.bn.weight\", \"net.p5.0.bn.bias\", \"net.p5.0.bn.running_mean\", \"net.p5.0.bn.running_var\", \"net.p5.1.cv1.conv.weight\", \"net.p5.1.cv1.bn.weight\", \"net.p5.1.cv1.bn.bias\", \"net.p5.1.cv1.bn.running_mean\", \"net.p5.1.cv1.bn.running_var\", \"net.p5.1.cv2.conv.weight\", \"net.p5.1.cv2.bn.weight\", \"net.p5.1.cv2.bn.bias\", \"net.p5.1.cv2.bn.running_mean\", \"net.p5.1.cv2.bn.running_var\", \"net.p5.1.m.0.cv1.conv.weight\", \"net.p5.1.m.0.cv1.bn.weight\", \"net.p5.1.m.0.cv1.bn.bias\", \"net.p5.1.m.0.cv1.bn.running_mean\", \"net.p5.1.m.0.cv1.bn.running_var\", \"net.p5.1.m.0.cv2.conv.weight\", \"net.p5.1.m.0.cv2.bn.weight\", \"net.p5.1.m.0.cv2.bn.bias\", \"net.p5.1.m.0.cv2.bn.running_mean\", \"net.p5.1.m.0.cv2.bn.running_var\", \"net.p5.2.cv1.conv.weight\", \"net.p5.2.cv1.bn.weight\", \"net.p5.2.cv1.bn.bias\", \"net.p5.2.cv1.bn.running_mean\", \"net.p5.2.cv1.bn.running_var\", \"net.p5.2.cv2.conv.weight\", \"net.p5.2.cv2.bn.weight\", \"net.p5.2.cv2.bn.bias\", \"net.p5.2.cv2.bn.running_mean\", \"net.p5.2.cv2.bn.running_var\", \"fpn.h1.cv1.conv.weight\", \"fpn.h1.cv1.bn.weight\", \"fpn.h1.cv1.bn.bias\", \"fpn.h1.cv1.bn.running_mean\", \"fpn.h1.cv1.bn.running_var\", \"fpn.h1.cv2.conv.weight\", \"fpn.h1.cv2.bn.weight\", \"fpn.h1.cv2.bn.bias\", \"fpn.h1.cv2.bn.running_mean\", \"fpn.h1.cv2.bn.running_var\", \"fpn.h1.m.0.cv1.conv.weight\", \"fpn.h1.m.0.cv1.bn.weight\", \"fpn.h1.m.0.cv1.bn.bias\", \"fpn.h1.m.0.cv1.bn.running_mean\", \"fpn.h1.m.0.cv1.bn.running_var\", \"fpn.h1.m.0.cv2.conv.weight\", \"fpn.h1.m.0.cv2.bn.weight\", \"fpn.h1.m.0.cv2.bn.bias\", \"fpn.h1.m.0.cv2.bn.running_mean\", \"fpn.h1.m.0.cv2.bn.running_var\", \"fpn.h2.cv1.conv.weight\", \"fpn.h2.cv1.bn.weight\", \"fpn.h2.cv1.bn.bias\", \"fpn.h2.cv1.bn.running_mean\", \"fpn.h2.cv1.bn.running_var\", \"fpn.h2.cv2.conv.weight\", \"fpn.h2.cv2.bn.weight\", \"fpn.h2.cv2.bn.bias\", \"fpn.h2.cv2.bn.running_mean\", \"fpn.h2.cv2.bn.running_var\", \"fpn.h2.m.0.cv1.conv.weight\", \"fpn.h2.m.0.cv1.bn.weight\", \"fpn.h2.m.0.cv1.bn.bias\", \"fpn.h2.m.0.cv1.bn.running_mean\", \"fpn.h2.m.0.cv1.bn.running_var\", \"fpn.h2.m.0.cv2.conv.weight\", \"fpn.h2.m.0.cv2.bn.weight\", \"fpn.h2.m.0.cv2.bn.bias\", \"fpn.h2.m.0.cv2.bn.running_mean\", \"fpn.h2.m.0.cv2.bn.running_var\", \"fpn.h3.conv.weight\", \"fpn.h3.bn.weight\", \"fpn.h3.bn.bias\", \"fpn.h3.bn.running_mean\", \"fpn.h3.bn.running_var\", \"fpn.h4.cv1.conv.weight\", \"fpn.h4.cv1.bn.weight\", \"fpn.h4.cv1.bn.bias\", \"fpn.h4.cv1.bn.running_mean\", \"fpn.h4.cv1.bn.running_var\", \"fpn.h4.cv2.conv.weight\", \"fpn.h4.cv2.bn.weight\", \"fpn.h4.cv2.bn.bias\", \"fpn.h4.cv2.bn.running_mean\", \"fpn.h4.cv2.bn.running_var\", \"fpn.h4.m.0.cv1.conv.weight\", \"fpn.h4.m.0.cv1.bn.weight\", \"fpn.h4.m.0.cv1.bn.bias\", \"fpn.h4.m.0.cv1.bn.running_mean\", \"fpn.h4.m.0.cv1.bn.running_var\", \"fpn.h4.m.0.cv2.conv.weight\", \"fpn.h4.m.0.cv2.bn.weight\", \"fpn.h4.m.0.cv2.bn.bias\", \"fpn.h4.m.0.cv2.bn.running_mean\", \"fpn.h4.m.0.cv2.bn.running_var\", \"fpn.h5.conv.weight\", \"fpn.h5.bn.weight\", \"fpn.h5.bn.bias\", \"fpn.h5.bn.running_mean\", \"fpn.h5.bn.running_var\", \"fpn.h6.cv1.conv.weight\", \"fpn.h6.cv1.bn.weight\", \"fpn.h6.cv1.bn.bias\", \"fpn.h6.cv1.bn.running_mean\", \"fpn.h6.cv1.bn.running_var\", \"fpn.h6.cv2.conv.weight\", \"fpn.h6.cv2.bn.weight\", \"fpn.h6.cv2.bn.bias\", \"fpn.h6.cv2.bn.running_mean\", \"fpn.h6.cv2.bn.running_var\", \"fpn.h6.m.0.cv1.conv.weight\", \"fpn.h6.m.0.cv1.bn.weight\", \"fpn.h6.m.0.cv1.bn.bias\", \"fpn.h6.m.0.cv1.bn.running_mean\", \"fpn.h6.m.0.cv1.bn.running_var\", \"fpn.h6.m.0.cv2.conv.weight\", \"fpn.h6.m.0.cv2.bn.weight\", \"fpn.h6.m.0.cv2.bn.bias\", \"fpn.h6.m.0.cv2.bn.running_mean\", \"fpn.h6.m.0.cv2.bn.running_var\", \"head.box.0.0.conv.weight\", \"head.box.0.0.bn.weight\", \"head.box.0.0.bn.bias\", \"head.box.0.0.bn.running_mean\", \"head.box.0.0.bn.running_var\", \"head.box.0.1.conv.weight\", \"head.box.0.1.bn.weight\", \"head.box.0.1.bn.bias\", \"head.box.0.1.bn.running_mean\", \"head.box.0.1.bn.running_var\", \"head.box.0.2.weight\", \"head.box.0.2.bias\", \"head.box.1.0.conv.weight\", \"head.box.1.0.bn.weight\", \"head.box.1.0.bn.bias\", \"head.box.1.0.bn.running_mean\", \"head.box.1.0.bn.running_var\", \"head.box.1.1.conv.weight\", \"head.box.1.1.bn.weight\", \"head.box.1.1.bn.bias\", \"head.box.1.1.bn.running_mean\", \"head.box.1.1.bn.running_var\", \"head.box.1.2.weight\", \"head.box.1.2.bias\", \"head.box.2.0.conv.weight\", \"head.box.2.0.bn.weight\", \"head.box.2.0.bn.bias\", \"head.box.2.0.bn.running_mean\", \"head.box.2.0.bn.running_var\", \"head.box.2.1.conv.weight\", \"head.box.2.1.bn.weight\", \"head.box.2.1.bn.bias\", \"head.box.2.1.bn.running_mean\", \"head.box.2.1.bn.running_var\", \"head.box.2.2.weight\", \"head.box.2.2.bias\", \"head.cls.0.0.conv.weight\", \"head.cls.0.0.bn.weight\", \"head.cls.0.0.bn.bias\", \"head.cls.0.0.bn.running_mean\", \"head.cls.0.0.bn.running_var\", \"head.cls.0.1.conv.weight\", \"head.cls.0.1.bn.weight\", \"head.cls.0.1.bn.bias\", \"head.cls.0.1.bn.running_mean\", \"head.cls.0.1.bn.running_var\", \"head.cls.0.2.weight\", \"head.cls.0.2.bias\", \"head.cls.1.0.conv.weight\", \"head.cls.1.0.bn.weight\", \"head.cls.1.0.bn.bias\", \"head.cls.1.0.bn.running_mean\", \"head.cls.1.0.bn.running_var\", \"head.cls.1.1.conv.weight\", \"head.cls.1.1.bn.weight\", \"head.cls.1.1.bn.bias\", \"head.cls.1.1.bn.running_mean\", \"head.cls.1.1.bn.running_var\", \"head.cls.1.2.weight\", \"head.cls.1.2.bias\", \"head.cls.2.0.conv.weight\", \"head.cls.2.0.bn.weight\", \"head.cls.2.0.bn.bias\", \"head.cls.2.0.bn.running_mean\", \"head.cls.2.0.bn.running_var\", \"head.cls.2.1.conv.weight\", \"head.cls.2.1.bn.weight\", \"head.cls.2.1.bn.bias\", \"head.cls.2.1.bn.running_mean\", \"head.cls.2.1.bn.running_var\", \"head.cls.2.2.weight\", \"head.cls.2.2.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m filtered_state_dict \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m state_dict\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mstate_dict()}\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 모델에 가중치 로드\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(filtered_state_dict)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch2/lib/python3.11/site-packages/torch/nn/modules/module.py:2152\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2147\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2148\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2149\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2153\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for YOLO:\n\tMissing key(s) in state_dict: \"net.p1.0.conv.weight\", \"net.p1.0.bn.weight\", \"net.p1.0.bn.bias\", \"net.p1.0.bn.running_mean\", \"net.p1.0.bn.running_var\", \"net.p2.0.conv.weight\", \"net.p2.0.bn.weight\", \"net.p2.0.bn.bias\", \"net.p2.0.bn.running_mean\", \"net.p2.0.bn.running_var\", \"net.p2.1.cv1.conv.weight\", \"net.p2.1.cv1.bn.weight\", \"net.p2.1.cv1.bn.bias\", \"net.p2.1.cv1.bn.running_mean\", \"net.p2.1.cv1.bn.running_var\", \"net.p2.1.cv2.conv.weight\", \"net.p2.1.cv2.bn.weight\", \"net.p2.1.cv2.bn.bias\", \"net.p2.1.cv2.bn.running_mean\", \"net.p2.1.cv2.bn.running_var\", \"net.p2.1.m.0.cv1.conv.weight\", \"net.p2.1.m.0.cv1.bn.weight\", \"net.p2.1.m.0.cv1.bn.bias\", \"net.p2.1.m.0.cv1.bn.running_mean\", \"net.p2.1.m.0.cv1.bn.running_var\", \"net.p2.1.m.0.cv2.conv.weight\", \"net.p2.1.m.0.cv2.bn.weight\", \"net.p2.1.m.0.cv2.bn.bias\", \"net.p2.1.m.0.cv2.bn.running_mean\", \"net.p2.1.m.0.cv2.bn.running_var\", \"net.p3.0.conv.weight\", \"net.p3.0.bn.weight\", \"net.p3.0.bn.bias\", \"net.p3.0.bn.running_mean\", \"net.p3.0.bn.running_var\", \"net.p3.1.cv1.conv.weight\", \"net.p3.1.cv1.bn.weight\", \"net.p3.1.cv1.bn.bias\", \"net.p3.1.cv1.bn.running_mean\", \"net.p3.1.cv1.bn.running_var\", \"net.p3.1.cv2.conv.weight\", \"net.p3.1.cv2.bn.weight\", \"net.p3.1.cv2.bn.bias\", \"net.p3.1.cv2.bn.running_mean\", \"net.p3.1.cv2.bn.running_var\", \"net.p3.1.m.0.cv1.conv.weight\", \"net.p3.1.m.0.cv1.bn.weight\", \"net.p3.1.m.0.cv1.bn.bias\", \"net.p3.1.m.0.cv1.bn.running_mean\", \"net.p3.1.m.0.cv1.bn.running_var\", \"net.p3.1.m.0.cv2.conv.weight\", \"net.p3.1.m.0.cv2.bn.weight\", \"net.p3.1.m.0.cv2.bn.bias\", \"net.p3.1.m.0.cv2.bn.running_mean\", \"net.p3.1.m.0.cv2.bn.running_var\", \"net.p3.1.m.1.cv1.conv.weight\", \"net.p3.1.m.1.cv1.bn.weight\", \"net.p3.1.m.1.cv1.bn.bias\", \"net.p3.1.m.1.cv1.bn.running_mean\", \"net.p3.1.m.1.cv1.bn.running_var\", \"net.p3.1.m.1.cv2.conv.weight\", \"net.p3.1.m.1.cv2.bn.weight\", \"net.p3.1.m.1.cv2.bn.bias\", \"net.p3.1.m.1.cv2.bn.running_mean\", \"net.p3.1.m.1.cv2.bn.running_var\", \"net.p4.0.conv.weight\", \"net.p4.0.bn.weight\", \"net.p4.0.bn.bias\", \"net.p4.0.bn.running_mean\", \"net.p4.0.bn.running_var\", \"net.p4.1.cv1.conv.weight\", \"net.p4.1.cv1.bn.weight\", \"net.p4.1.cv1.bn.bias\", \"net.p4.1.cv1.bn.running_mean\", \"net.p4.1.cv1.bn.running_var\", \"net.p4.1.cv2.conv.weight\", \"net.p4.1.cv2.bn.weight\", \"net.p4.1.cv2.bn.bias\", \"net.p4.1.cv2.bn.running_mean\", \"net.p4.1.cv2.bn.running_var\", \"net.p4.1.m.0.cv1.conv.weight\", \"net.p4.1.m.0.cv1.bn.weight\", \"net.p4.1.m.0.cv1.bn.bias\", \"net.p4.1.m.0.cv1.bn.running_mean\", \"net.p4.1.m.0.cv1.bn.running_var\", \"net.p4.1.m.0.cv2.conv.weight\", \"net.p4.1.m.0.cv2.bn.weight\", \"net.p4.1.m.0.cv2.bn.bias\", \"net.p4.1.m.0.cv2.bn.running_mean\", \"net.p4.1.m.0.cv2.bn.running_var\", \"net.p4.1.m.1.cv1.conv.weight\", \"net.p4.1.m.1.cv1.bn.weight\", \"net.p4.1.m.1.cv1.bn.bias\", \"net.p4.1.m.1.cv1.bn.running_mean\", \"net.p4.1.m.1.cv1.bn.running_var\", \"net.p4.1.m.1.cv2.conv.weight\", \"net.p4.1.m.1.cv2.bn.weight\", \"net.p4.1.m.1.cv2.bn.bias\", \"net.p4.1.m.1.cv2.bn.running_mean\", \"net.p4.1.m.1.cv2.bn.running_var\", \"net.p5.0.conv.weight\", \"net.p5.0.bn.weight\", \"net.p5.0.bn.bias\", \"net.p5.0.bn.running_mean\", \"net.p5.0.bn.running_var\", \"net.p5.1.cv1.conv.weight\", \"net.p5.1.cv1.bn.weight\", \"net.p5.1.cv1.bn.bias\", \"net.p5.1.cv1.bn.running_mean\", \"net.p5.1.cv1.bn.running_var\", \"net.p5.1.cv2.conv.weight\", \"net.p5.1.cv2.bn.weight\", \"net.p5.1.cv2.bn.bias\", \"net.p5.1.cv2.bn.running_mean\", \"net.p5.1.cv2.bn.running_var\", \"net.p5.1.m.0.cv1.conv.weight\", \"net.p5.1.m.0.cv1.bn.weight\", \"net.p5.1.m.0.cv1.bn.bias\", \"net.p5.1.m.0.cv1.bn.running_mean\", \"net.p5.1.m.0.cv1.bn.running_var\", \"net.p5.1.m.0.cv2.conv.weight\", \"net.p5.1.m.0.cv2.bn.weight\", \"net.p5.1.m.0.cv2.bn.bias\", \"net.p5.1.m.0.cv2.bn.running_mean\", \"net.p5.1.m.0.cv2.bn.running_var\", \"net.p5.2.cv1.conv.weight\", \"net.p5.2.cv1.bn.weight\", \"net.p5.2.cv1.bn.bias\", \"net.p5.2.cv1.bn.running_mean\", \"net.p5.2.cv1.bn.running_var\", \"net.p5.2.cv2.conv.weight\", \"net.p5.2.cv2.bn.weight\", \"net.p5.2.cv2.bn.bias\", \"net.p5.2.cv2.bn.running_mean\", \"net.p5.2.cv2.bn.running_var\", \"fpn.h1.cv1.conv.weight\", \"fpn.h1.cv1.bn.weight\", \"fpn.h1.cv1.bn.bias\", \"fpn.h1.cv1.bn.running_mean\", \"fpn.h1.cv1.bn.running_var\", \"fpn.h1.cv2.conv.weight\", \"fpn.h1.cv2.bn.weight\", \"fpn.h1.cv2.bn.bias\", \"fpn.h1.cv2.bn.running_mean\", \"fpn.h1.cv2.bn.running_var\", \"fpn.h1.m.0.cv1.conv.weight\", \"fpn.h1.m.0.cv1.bn.weight\", \"fpn.h1.m.0.cv1.bn.bias\", \"fpn.h1.m.0.cv1.bn.running_mean\", \"fpn.h1.m.0.cv1.bn.running_var\", \"fpn.h1.m.0.cv2.conv.weight\", \"fpn.h1.m.0.cv2.bn.weight\", \"fpn.h1.m.0.cv2.bn.bias\", \"fpn.h1.m.0.cv2.bn.running_mean\", \"fpn.h1.m.0.cv2.bn.running_var\", \"fpn.h2.cv1.conv.weight\", \"fpn.h2.cv1.bn.weight\", \"fpn.h2.cv1.bn.bias\", \"fpn.h2.cv1.bn.running_mean\", \"fpn.h2.cv1.bn.running_var\", \"fpn.h2.cv2.conv.weight\", \"fpn.h2.cv2.bn.weight\", \"fpn.h2.cv2.bn.bias\", \"fpn.h2.cv2.bn.running_mean\", \"fpn.h2.cv2.bn.running_var\", \"fpn.h2.m.0.cv1.conv.weight\", \"fpn.h2.m.0.cv1.bn.weight\", \"fpn.h2.m.0.cv1.bn.bias\", \"fpn.h2.m.0.cv1.bn.running_mean\", \"fpn.h2.m.0.cv1.bn.running_var\", \"fpn.h2.m.0.cv2.conv.weight\", \"fpn.h2.m.0.cv2.bn.weight\", \"fpn.h2.m.0.cv2.bn.bias\", \"fpn.h2.m.0.cv2.bn.running_mean\", \"fpn.h2.m.0.cv2.bn.running_var\", \"fpn.h3.conv.weight\", \"fpn.h3.bn.weight\", \"fpn.h3.bn.bias\", \"fpn.h3.bn.running_mean\", \"fpn.h3.bn.running_var\", \"fpn.h4.cv1.conv.weight\", \"fpn.h4.cv1.bn.weight\", \"fpn.h4.cv1.bn.bias\", \"fpn.h4.cv1.bn.running_mean\", \"fpn.h4.cv1.bn.running_var\", \"fpn.h4.cv2.conv.weight\", \"fpn.h4.cv2.bn.weight\", \"fpn.h4.cv2.bn.bias\", \"fpn.h4.cv2.bn.running_mean\", \"fpn.h4.cv2.bn.running_var\", \"fpn.h4.m.0.cv1.conv.weight\", \"fpn.h4.m.0.cv1.bn.weight\", \"fpn.h4.m.0.cv1.bn.bias\", \"fpn.h4.m.0.cv1.bn.running_mean\", \"fpn.h4.m.0.cv1.bn.running_var\", \"fpn.h4.m.0.cv2.conv.weight\", \"fpn.h4.m.0.cv2.bn.weight\", \"fpn.h4.m.0.cv2.bn.bias\", \"fpn.h4.m.0.cv2.bn.running_mean\", \"fpn.h4.m.0.cv2.bn.running_var\", \"fpn.h5.conv.weight\", \"fpn.h5.bn.weight\", \"fpn.h5.bn.bias\", \"fpn.h5.bn.running_mean\", \"fpn.h5.bn.running_var\", \"fpn.h6.cv1.conv.weight\", \"fpn.h6.cv1.bn.weight\", \"fpn.h6.cv1.bn.bias\", \"fpn.h6.cv1.bn.running_mean\", \"fpn.h6.cv1.bn.running_var\", \"fpn.h6.cv2.conv.weight\", \"fpn.h6.cv2.bn.weight\", \"fpn.h6.cv2.bn.bias\", \"fpn.h6.cv2.bn.running_mean\", \"fpn.h6.cv2.bn.running_var\", \"fpn.h6.m.0.cv1.conv.weight\", \"fpn.h6.m.0.cv1.bn.weight\", \"fpn.h6.m.0.cv1.bn.bias\", \"fpn.h6.m.0.cv1.bn.running_mean\", \"fpn.h6.m.0.cv1.bn.running_var\", \"fpn.h6.m.0.cv2.conv.weight\", \"fpn.h6.m.0.cv2.bn.weight\", \"fpn.h6.m.0.cv2.bn.bias\", \"fpn.h6.m.0.cv2.bn.running_mean\", \"fpn.h6.m.0.cv2.bn.running_var\", \"head.box.0.0.conv.weight\", \"head.box.0.0.bn.weight\", \"head.box.0.0.bn.bias\", \"head.box.0.0.bn.running_mean\", \"head.box.0.0.bn.running_var\", \"head.box.0.1.conv.weight\", \"head.box.0.1.bn.weight\", \"head.box.0.1.bn.bias\", \"head.box.0.1.bn.running_mean\", \"head.box.0.1.bn.running_var\", \"head.box.0.2.weight\", \"head.box.0.2.bias\", \"head.box.1.0.conv.weight\", \"head.box.1.0.bn.weight\", \"head.box.1.0.bn.bias\", \"head.box.1.0.bn.running_mean\", \"head.box.1.0.bn.running_var\", \"head.box.1.1.conv.weight\", \"head.box.1.1.bn.weight\", \"head.box.1.1.bn.bias\", \"head.box.1.1.bn.running_mean\", \"head.box.1.1.bn.running_var\", \"head.box.1.2.weight\", \"head.box.1.2.bias\", \"head.box.2.0.conv.weight\", \"head.box.2.0.bn.weight\", \"head.box.2.0.bn.bias\", \"head.box.2.0.bn.running_mean\", \"head.box.2.0.bn.running_var\", \"head.box.2.1.conv.weight\", \"head.box.2.1.bn.weight\", \"head.box.2.1.bn.bias\", \"head.box.2.1.bn.running_mean\", \"head.box.2.1.bn.running_var\", \"head.box.2.2.weight\", \"head.box.2.2.bias\", \"head.cls.0.0.conv.weight\", \"head.cls.0.0.bn.weight\", \"head.cls.0.0.bn.bias\", \"head.cls.0.0.bn.running_mean\", \"head.cls.0.0.bn.running_var\", \"head.cls.0.1.conv.weight\", \"head.cls.0.1.bn.weight\", \"head.cls.0.1.bn.bias\", \"head.cls.0.1.bn.running_mean\", \"head.cls.0.1.bn.running_var\", \"head.cls.0.2.weight\", \"head.cls.0.2.bias\", \"head.cls.1.0.conv.weight\", \"head.cls.1.0.bn.weight\", \"head.cls.1.0.bn.bias\", \"head.cls.1.0.bn.running_mean\", \"head.cls.1.0.bn.running_var\", \"head.cls.1.1.conv.weight\", \"head.cls.1.1.bn.weight\", \"head.cls.1.1.bn.bias\", \"head.cls.1.1.bn.running_mean\", \"head.cls.1.1.bn.running_var\", \"head.cls.1.2.weight\", \"head.cls.1.2.bias\", \"head.cls.2.0.conv.weight\", \"head.cls.2.0.bn.weight\", \"head.cls.2.0.bn.bias\", \"head.cls.2.0.bn.running_mean\", \"head.cls.2.0.bn.running_var\", \"head.cls.2.1.conv.weight\", \"head.cls.2.1.bn.weight\", \"head.cls.2.1.bn.bias\", \"head.cls.2.1.bn.running_mean\", \"head.cls.2.1.bn.running_var\", \"head.cls.2.2.weight\", \"head.cls.2.2.bias\". "
     ]
    }
   ],
   "source": [
    "model = yolo_v8_n()\n",
    "\n",
    "# 저장된 가중치 로드\n",
    "state_dict = torch.load('yolov8n.pt')\n",
    "\n",
    "# 모델의 state_dict에서 필요한 가중치만 추출\n",
    "filtered_state_dict = {k: v for k, v in state_dict.items() if k in model.state_dict()}\n",
    "\n",
    "# 모델에 가중치 로드\n",
    "model.load_state_dict(filtered_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load\n",
    "\n",
    "with open('/mnt/d/projects/objdet/src/objdet/YOLOV8/weights/yolov8n-coco.safetensors', 'rb') as f:\n",
    "    state_dict = load(f.read())\n",
    "    \n",
    "model = yolo_v8_n()\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import cv2 \n",
    "import numpy as np\n",
    "\n",
    "m = YOLO('yolov8n.pt')\n",
    "\n",
    "m_model = m.model\n",
    "m_model.eval()\n",
    "\n",
    "image_path = '/mnt/d/projects/objdet/src/objdet/samples/sample.png'\n",
    "im = cv2.imread(image_path)\n",
    "im0 = im.copy()\n",
    "im = im.astype(np.float32)[:,:,::-1]\n",
    "im = im/255\n",
    "im = im.transpose((2,0,1))\n",
    "im = torch.from_numpy(im).unsqueeze(0)\n",
    "\n",
    "pred = m_model(im)\n",
    "\n",
    "array = pred[0].reshape(-1,6)\n",
    "boxes = array[:, :4]  # Bounding box coordinates\n",
    "confidences = array[:, 4]  # Confidence scores\n",
    "class_scores = array[:, 5]  # Class scores\n",
    "# Assume 'boxes', 'confidences', and 'class_scores' are already defined\n",
    "indices = cv2.dnn.NMSBoxes(boxes.tolist(), confidences.tolist(), score_threshold=0.01, nms_threshold=0.01)\n",
    "\n",
    "final_boxes = boxes[indices].reshape(-1, 4)\n",
    "final_confidences = confidences[indices]\n",
    "final_class_scores = class_scores[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('/mnt/d/projects/objdet/src/objdet/samples/sample.png')\n",
    "\n",
    "# 각 박스를 이미지 위에 그립니다.\n",
    "for box in final_boxes:\n",
    "    cv2.rectangle(img, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (0, 255, 0), 2)  # 초록색으로 박스 그리기\n",
    "\n",
    "# 결과를 화면에 표시합니다.\n",
    "cv2.imshow('Image with Boxes', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 84, 9072])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 144, 72, 96])\n",
      "torch.Size([1, 144, 36, 48])\n",
      "torch.Size([1, 144, 18, 24])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 576, 768])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for p in pred[1]:\n",
    "    print(p.shape)\n",
    "im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "image_path = '/mnt/d/projects/objdet/src/objdet/samples/sample.png'\n",
    "im = cv2.imread(image_path)\n",
    "im0 = im.copy()\n",
    "im = im.astype(np.float32)[:,:,::-1]\n",
    "im = im/255\n",
    "im = im.transpose((2,0,1))\n",
    "im = torch.from_numpy(im).unsqueeze(0)\n",
    "\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "pred = model(im)\n",
    "\n",
    "# model.eval()\n",
    "result2 = model(im)\n",
    "array = result2[0].reshape(-1,6)\n",
    "boxes = array[:, :4]  # Bounding box coordinates\n",
    "confidences = array[:, 4]  # Confidence scores\n",
    "class_scores = array[:, 5]  # Class scores\n",
    "# Assume 'boxes', 'confidences', and 'class_scores' are already defined\n",
    "indices = cv2.dnn.NMSBoxes(boxes.tolist(), confidences.tolist(), score_threshold=0.5, nms_threshold=0.4)\n",
    "\n",
    "final_boxes = boxes[indices].reshape(-1, 4)\n",
    "final_confidences = confidences[indices]\n",
    "final_class_scores = class_scores[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# 이미지를 로드합니다.\n",
    "img = cv2.imread('/mnt/d/projects/objdet/src/objdet/samples/sample.png')\n",
    "\n",
    "# 각 박스를 이미지 위에 그립니다.\n",
    "for box in final_boxes:\n",
    "    cv2.rectangle(img, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (0, 255, 0), 2)  # 초록색으로 박스 그리기\n",
    "\n",
    "# 결과를 화면에 표시합니다.\n",
    "cv2.imshow('Image with Boxes', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 576, 768])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "m = YOLO('yolov8n.pt')\n",
    "\n",
    "# m.eval()\n",
    "result = m('/mnt/d/projects/objdet/src/objdet/samples/sample.png')\n",
    "array = result[0].reshape(-1,6)\n",
    "boxes = array[:, :4]  # Bounding box coordinates\n",
    "confidences = array[:, 4]  # Confidence scores\n",
    "class_scores = array[:, 5]  # Class scores\n",
    "\n",
    "# Assume 'boxes', 'confidences', and 'class_scores' are already defined\n",
    "indices = cv2.dnn.NMSBoxes(boxes.tolist(), confidences.tolist(), score_threshold=0.5, nms_threshold=0.4)\n",
    "\n",
    "final_boxes = boxes[indices].reshape(-1, 4)\n",
    "final_confidences = confidences[indices]\n",
    "final_class_scores = class_scores[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# 이미지를 로드합니다.\n",
    "img = cv2.imread('/mnt/d/projects/objdet/src/objdet/samples/sample.png')\n",
    "\n",
    "# 각 박스를 이미지 위에 그립니다.\n",
    "for box in final_boxes:\n",
    "    cv2.rectangle(img, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (0, 255, 0), 2)  # 초록색으로 박스 그리기\n",
    "\n",
    "# 결과를 화면에 표시합니다.\n",
    "cv2.imshow('Image with Boxes', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
